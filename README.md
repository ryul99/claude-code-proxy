# Anthropic API Proxy for any LLM via LiteLLM ğŸ”„

**Use Anthropic clients (like Claude Code) with any LLM backend.** ğŸ¤

A proxy server that lets you use Anthropic clients with any LLM provider supported by LiteLLM. ğŸŒ‰

![Anthropic API Proxy](pic.png)

## Quick Start âš¡

### Prerequisites

- An API key for your chosen LLM provider (e.g., OpenAI, Google, Anthropic, Cohere, etc.) ğŸ”‘
- [uv](https://github.com/astral-sh/uv) installed.

### Setup

Install the proxy server using one of the following methods:

```bash
uv tool install 'git+https://github.com/ryul99/claude-code-proxy.git'
# or
pipx install 'git+https://github.com/ryul99/claude-code-proxy.git'
# or
pip install 'git+https://github.com/ryul99/claude-code-proxy.git'
```

And run the proxy server:

```bash
BIG_MODEL=openai/gpt-4.1 SMALL_MODEL=openai/gpt-4.1 claude-proxy
```

That's it! The proxy server will start listening on port 8082 by default. You can change the port by setting `--port` option

### Using with Claude Code

1.  **Install Claude Code** (if you haven't already):
    ```bash
    npm install -g @anthropic-ai/claude-code
    ```

2.  **Connect to your proxy**:
    ```bash
    ANTHROPIC_BASE_URL=http://localhost:8082 claude
    ```

3.  **That's it!** Your Claude Code client will now use the configured backend models through the proxy. ğŸ¯

## Model Mapping

The proxy maps Anthropic's `sonnet` and `haiku` models to any LiteLLM-supported model string specified in your environment variables.

- `Claude Sonnet` -> `BIG_MODEL`
- `Claude Haiku`  -> `SMALL_MODEL`

You **must** use the correct model identifier as required by LiteLLM, which typically includes the provider prefix (e.g., `openai/`, `gemini/`, `anthropic/`).

### Customizing Model Mapping

Control the mapping by setting the environment variables

**Example 1: Use OpenAI**
```bash
export LLM_API_KEY="your-openai-key"
BIG_MODEL="openai/gpt-4o" SMALL_MODEL="openai/gpt-4o-mini" claude-proxy
```

**Example 2: Use Google Models**
```bash
export LLM_API_KEY="your-google-key"
BIG_MODEL="gemini/gemini-1.5-pro-latest" SMALL_MODEL="gemini/gemini-1.5-flash-latest" claude-proxy
```

**Example 3: Use Anthropic Models**
```bash
LLM_API_KEY="your-anthropic-key"
BIG_MODEL="anthropic/claude-3-5-sonnet-20240620" SMALL_MODEL="anthropic/claude-3-haiku-20240307"
```

For more supported models and provider-specific environment variables (like `AZURE_API_BASE`, etc.), refer to the [LiteLLM documentation](https://docs.litellm.ai/).

## How It Works ğŸ§©

This proxy works by:

1.  **Receiving requests** in Anthropic's API format ğŸ“¥
2.  **Translating** the requests to a generic format via LiteLLM ğŸ”„
3.  **Sending** the translated request to your configured LLM provider ğŸ“¤
4.  **Converting** the response back to Anthropic format ğŸ”„
5.  **Returning** the formatted response to the client âœ…

The proxy handles both streaming and non-streaming responses, maintaining compatibility with all Claude clients. ğŸŒŠ

## Contributing ğŸ¤

Contributions are welcome! Please feel free to submit a Pull Request. ğŸ

## Pre-commit Setup ğŸ”§

This project uses pre-commit hooks to ensure code quality. To set up pre-commit hooks, run:

```bash
pip install pre-commit
pre-commit install
```

This will automatically run linting and formatting checks before each commit.
